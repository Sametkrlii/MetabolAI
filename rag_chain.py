# RAG_CHAIN.PY | LLM ve RAG ZÄ°NCÄ°RÄ° MODÃœLÃœ
#
# AmaÃ§:
# 1. Gemini API'ye baÄŸlantÄ±yÄ± kurar.
# 2. ChromaDB veritabanÄ±nÄ± yÃ¼kler.
# 3. KullanÄ±cÄ± sorusunu alÄ±r ve RAG zincirini kullanarak TÃ¼rkÃ§e Ã¶neri/cevap Ã¼retir.

from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import SentenceTransformerEmbeddings
import os



#Ã‡EVÄ°RÄ° FONKSÄ°YONU

def translate_query(query:str)->str:
    """"
    TÃ¼rkÃ§e sorguyu Gemini kullanarak Ä°ngilizceye Ã§evirir.
    """

    try:

        llm_translate=GoogleGenerativeAI(model="gemini-2.5-flash",temperature=0) #yaratÄ±cÄ±lÄ±k 0

        #Modele sadece Ã§eviri yapmasÄ±nÄ± sÃ¶ylÃ¼yoruz. **gemini ile gÃ¼Ã§lendirilmiÅŸ prompt.
        prompt = (
            f"AÅŸaÄŸÄ±daki TÃ¼rkÃ§e metni, sadece veritabanÄ±nda arama yapmak iÃ§in kullanÄ±lacak "
            f"NET Ä°NGÄ°LÄ°ZCE BESÄ°N ADI veya bir Ä°NGÄ°LÄ°ZCE SORGUSU olarak Ã§evir. "
            f"Ekstra cÃ¼mle kurma, sadece Ã§eviriyi dÃ¶ndÃ¼r: '{query}'"
        )

        #LLM i Ã§aÄŸÄ±rÄ±p sonucu alÄ±yoruz.
        response=llm_translate.invoke(prompt)

        return response.strip()
    
    except Exception as e:
        print(f"Ã‡eviri hatasÄ±:{e}. Orjinal Sorgu KullanÄ±lÄ±yor.(yanlÄ±ÅŸ sonuÃ§lar doÄŸurabilir.)")

        return query




# 1. VeritabanÄ± YÃ¼klemesi ve Retriever (Geri Ã‡aÄŸÄ±rÄ±cÄ±) HazÄ±rlÄ±ÄŸÄ±

CHROMA_DB_PATH="./chroma_db"

def get_retriever():
    #veri tabanÄ± kurarken kullandÄ±ÄŸÄ±mla aynÄ± olmalÄ±.
    embeddings_model=SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")

    #veritabanÄ±nÄ± diskten okuyup yÃ¼kleme
    vectorstore=Chroma(
        persist_directory=CHROMA_DB_PATH,
        embedding_function=embeddings_model
    )

    # Retriever objesi, LLM'in kullanacaÄŸÄ± arama aracÄ±dÄ±r, search_kwargs={"k":3} ifadesi, arama yapÄ±ldÄ±ÄŸÄ±nda anlamsal olarak en yakÄ±n 3 besini getirir.
    retriever=vectorstore.as_retriever(search_kwargs={"k":5})
    
    return retriever


# ----------------------------------------------------
# 2. Prompt Engineering (Modelin KimliÄŸini Belirleme)


#geminiye kiÅŸilik atayalÄ±m.
custom_prompt_template="""
Sen, kullanÄ±cÄ±ya kalori ve makro takibi konusunda yardÄ±mcÄ± olan, fitness odaklÄ±, akÄ±llÄ± bir Beslenme KoÃ§usun.
YanÄ±tlarÄ±n TÃ¼rkÃ§e, pozitif ve motivasyonel olmalÄ±dÄ±r. Motivasyonel olduÄŸu kadar gerÃ§ekÃ§i de olmalÄ± ki karÅŸÄ±ndaki kiÅŸi doÄŸru bilgiler edinebilsin.

GÃ¶revler:
1. YALNIZCA saÄŸladÄ±ÄŸÄ±m BESÄ°N BÄ°LGÄ°LERÄ°NÄ° (CONTEXT) kullan. Asla uydurma veya genel internet bilgisi verme.
2. CONTEXT iÃ§indeki TÃœM YÄ°YECEK ADLARINI TÃ¼rkÃ§eye Ã§evirerek (Ã–rn: Cupcake yerine 'KÃ¼Ã§Ã¼k Kek', Butternaan yerine 'TereyaÄŸlÄ± Bazlana' gibi) kullanÄ±cÄ±ya sun. YabancÄ± kelime kullanma.
3. CevabÄ± kÄ±sa, net ve anlaÅŸÄ±lÄ±r bir TÃ¼rkÃ§e formatta Ã¼ret.

KullanÄ±cÄ±nÄ±n Sorgusu: {question}
BESÄ°N BÄ°LGÄ°LERÄ° (CONTEXT): {context}

YANITIN (TÃ¼rkÃ§e ve Beslenme KoÃ§u tarzÄ±nda):
"""


RAG_PROMPT=PromptTemplate(
    template=custom_prompt_template,
    input_variables=["context","question"]
)

def get_base_llm():
    """Gemini LLM instance'Ä±nÄ± dÃ¶ndÃ¼rÃ¼r"""
    return ChatGoogleGenerativeAI(model="gemini-2.5-flash-exp",temperature=0.2)



# RAG CHAIN KURULUMU

def setup_rag_chain():
    """
    Gemini LLM'i ve ChromaDB Retriever'Ä± birleÅŸtirerek RAG zincirini kurar.
    """
    # 1. LLM'i tanÄ±mlama
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash",
        temperature=0.2 
    )
    
    # 2. Retriever'Ä± yÃ¼kleme
    retriever = get_retriever()

    # 3. RetrievalQA Zincirini kurma (RAG'in Kalbi)
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff", # Ã‡ekilen tÃ¼m context'i modele gÃ¶nder demektir.
        retriever=retriever,
        return_source_documents=False,
        chain_type_kwargs={"prompt": RAG_PROMPT}
    )
    return qa_chain



# ----------------------------------------------------
# 3. Test BloÄŸu
# ----------------------------------------------------


if __name__ == "__main__":
    from langchain_community.vectorstores import Chroma
    from langchain_community.embeddings import SentenceTransformerEmbeddings
    
    print("ğŸ”§ ChromaDB Test Ediliyor...\n")
    
    # VeritabanÄ±nÄ± yÃ¼kle
    embeddings = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")
    vectorstore = Chroma(
        persist_directory="./chroma_db",
        embedding_function=embeddings
    )
    
    # FarklÄ± sorgular test et
    test_queries = [
        "high protein low fat",
        "chicken breast",
        "30g protein 10g fat",
        "egg white",
        "fish salmon"
    ]
    
    for query in test_queries:
        print(f"\nğŸ“ Sorgu: '{query}'")
        results = vectorstore.similarity_search(query, k=3)
        
        for i, doc in enumerate(results, 1):
            # Ä°lk satÄ±rÄ± al (besin adÄ±)
            food_name = doc.page_content.split('\n')[0].replace('Besin AdÄ±: ', '')
            print(f"   {i}. {food_name}")